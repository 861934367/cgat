@c ################################################################################
@c #   Gene prediction pipeline 
@c #
@c #   $Id: gpipe.texi 20 2005-08-09 15:34:41Z andreas $
@c #
@c #   Copyright (C) 2004 Andreas Heger
@c #
@c #   This program is free software; you can redistribute it and/or
@c #   modify it under the terms of the GNU General Public License
@c #   as published by the Free Software Foundation; either version 2
@c #   of the License, or (at your option) any later version.
@c #
@c #   This program is distributed in the hope that it will be useful,
@c #   but WITHOUT ANY WARRANTY; without even the implied warranty of
@c #   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
@c #   GNU General Public License for more details.
@c #
@c #   You should have received a copy of the GNU General Public License
@c #   along with this program; if not, write to the Free Software
@c #   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
@c #################################################################################
\input texinfo   @c -*-texinfo-*-
@c %**start of header
@setfilename gpipe.info
@settitle Geneprediction pipeline, Chris Ponting's group.
@c %**end of header

@setchapternewpage odd

@ifinfo
A makefile to run gene prediction programs on a set of genomic files.

Copyright @copyright{} Andreas Heger
@end ifinfo

@titlepage
@sp 10
@center @titlefont{Gene prediction by homology}

@page
@vskip 0pt plus 1filll
Copyright @copyright{} 2004 Andreas Heger
@end titlepage

@node Top, Overview, (dir), (dir)
@chapter Top

@menu
* Overview::                    
* Installation::                
* Using the pipeline::          
* Analysing the result::        
* Implementation::              
* Command and Variable Index::  
* Concept Index::               

@detailmenu
 --- The Detailed Node Listing ---

Overview

* Reading::                     On reading this text
* Bugs::                        Problems and bugs

Installation

* Requirements::                Requirements for the software
* Obtaining the software::      How to set up the software
* Configuring the environment::  Configuration of the enviroment
* Configuring the pipeline::    
* Preparing the data::          

Using the pipeline

* Overview of Usage::           
* Preparation::                 
* Running the pipeline::        

Running the pipeline

* Step1::                       Masking of the protein sequences  
* Step2::                       Clustering of the protein sequences  
* Step3::                       Running exonerate    
* Step4::                       Running TBLASTN      
* Step5::                       Collating putative genic regions.  
* Step6::                       Predicting genes for representative sequences  
* Step7::                       Predicting genes for redundant sequences  
* Step8::                       Predicting genes for member sequences  
* Step9::                       Analysing the predictions  
* Step10::                      Quality control of predictions  

Analysing the results

* Troubleshooting::             

@end detailmenu
@end menu

@c ------------------------------------------------------
@node Overview, Installation, Top, Top
@chapter Overview

This document describes the pipeline of the Chris Ponting group
for predicting genes by homology. The input is a set of known transcripts
from a reference genome and the masked genomic sequence of a
target genome.

The pipeline predicts genes in a two-step procedure:

@enumerate
@item
Regions of similarity between the known transcripts and the reference genome 
are identified using a quick heuristic search.

@item
The regions of similarity of step 1 are submitted to a sensitive, but slower gene 
prediction program.
@end enumerate

The pipeline contains many options to mask sequences, analyse and quality
control the predictions and store the results in a relational database.
This manual describes how to setup up the pipeline, run it on our cluster,
and analyse the results.

@menu
* Reading::                     On reading this text
* Bugs::                        Problems and bugs
@end menu

@node Reading, Bugs, Overview, Overview
@section Reading

@node Bugs,  , Reading, Overview
@section  Bugs

Plenty.

@cindex Sample index entry

This is the contents of the first chapter.
@cindex Another sample index entry

Here is a numbered list.

The @code{makeinfo} and @code{texinfo-format-buffer}
commands transform a Texinfo file such as this into
an Info file; and @TeX{} typesets it for a printed
manual.


@c ------------------------------------------------------
@node Installation, Using the pipeline, Overview, Top
@chapter Installation

The pipeline consists of a set of scripts and a makefile,
that glues together the various scripts. This section tells
you what programs are needed to be installed @ref{Requirements}
and how to install the software from the pipeline @xref{Obtaining the software}.

Before starting the pipeline, you need to configure your environment
@ref{Configuring the environment} and the pipeline 
@xref{Configuring the pipeline}. 

Finally, the input files need to prepared @xref{Preparing the data}.

@menu
* Requirements::                Requirements for the software
* Obtaining the software::      How to set up the software
* Configuring the environment::  Configuration of the enviroment
* Configuring the pipeline::    
* Preparing the data::          
@end menu

@c -------------------------------------------------------------------------
@node Requirements, Obtaining the software, Installation, Installation
@section Requirements

This pipeline requires that several programs reside in your path. They have to
be in your path both on the submit host and the cluster nodes.

@enumerate
@item
exonerate: exonerate is a program to align a peptide sequence to a genomic sequence 
(other alignment modes are possible). It offers heuristic modes, that allow for fast 
scanning of large chunks of genomic DNA, and exhaustive modes, that do a full dynamic
programming mode.
@item
genewise (optional): genewise is a program to align a peptide sequence to a genomic sequence.
It uses full dynamic programming for the alignment and thus is quite slow.
@item
cd-hit: a program to cluster protein sequences based on their sequence identity. It is used
to reduce the input set needed for scanning.
@item
seg: a program to mask low complexity regions in protein sequences.
@item
seq_pairs_kaks: Leo's wrapper around codeml.
@item
map_cdna_onto_aa: Leo's program to map a cdna-sequence onto an amino acid
sequence.
@item
alignlib: a library for sequence alignments and its python interface
@item
python: the one and only.
@item
Gnuplot.py: a python interface to gnuplot
@item
Postgresql: a relational database
@item
Various python scripts/libraries from the general python toolbox.


@end enumerate

@c -------------------------------------------------------------------------
@node Obtaining the software, Configuring the environment, Requirements, Installation
@section Obtaining the software

The makefile is availabe via @xref{Top, CVS, , cvs}. The code can
be obtained by issuing a

@code{cvs -d $CVSROOT co gpipe}

where @var{$CVSROOT} is the directory of the CVS repository.
This will checkout the latest version of the pipeline. It will create 
a directory @file{gpipe} in the current directory that contains the
master makefile and all the scripts.

If you have your local copy of the source, you can run a

@code{cvs -d $CVSROOT update}

in the directory gpipe to update to the latest version.

Most of the required software has been installed under /net/cpp-group, so
there should be no need to install these separately. However, you need
to modify your environment @xref{Configuring the environment}, 
so that they are picked up correctly.

@c ------------------------------------------------------
@node Configuring the environment, Configuring the pipeline, Obtaining the software, Installation
@section Configuring the environment

A few environment variables have to be set in order to use the pipeline. 
Sourcing the file @file{gpipe.bash} in the working directory
should be enough:

@verbatim
source ./gpipe.bash
@end verbatim

The script sets/modifies the following environment variables:

@table @var
@item PATH
Add path to binary executables (@file{/net/cpp-group/bin}).
@item LD_LIBRARY_PATH
Add path to alignlib.so (@file{/net/cpp-group/lib}).
@item PTYHONPATH
Add path to python libraries (@file{/net/cpp-group/lib/python})
@end table

Note: the pipeline requires you to use bash! Two functions are
added to your bash enviroment:

@verbatim
#-----------------------------------
# helper functions for detecting errors in pipes
#-----------------------------------
detect_pipe_error_helper()
{
    while [ "$#" != 0 ] ; do
        # there was an error in at least one program of the pipe
        if [ "$1" != 0 ] ; then return 1 ; fi
        shift 1
    done
    return 0
}

detect_pipe_error()
{
    detect_pipe_error_helper "${PIPESTATUS[@]}"
    return $?
}
@end verbatim

@c ------------------------------------------------------
@node Configuring the pipeline, Preparing the data, Configuring the environment, Installation
@section Configuring the pipeline

For running the pipeline you need to create a working directory.
You can use the script @file{install.sh} to create a working directory 
and create a template Makefile in the working directory.

For example,
@verbatim
./install.bash /data/projects/myname/orga_vs_orgb
@end verbatim

will create the directory orga_vs_orgb in myname. If myname does not exist,
it will be created as well (and so on). I recommend using a local hard-disc
on one of the submit hosts for running the pipeline in order to reduce the
load on our network.

The optional second parameter to the install script specifies
a shared temporary directory that is accessible to both the submit host and the
cluster nodes. If no parameter is given, the default filename is
@file{/net/cpp-group/gpipe/tmp/working directory}. In the example above,
this would be /net/cpp-group/gpipe/tmp/data/projects/myname/orga_vs_orgb

The pipeline is quite configurable. The @file{Makefile} contains all
the parameters that can be set together with a description of the
parameters.

@node Preparing the data,  , Configuring the pipeline, Installation
@section Preparing the data

The pipeline needs four input files.

@table @file
@item peptides.fasta
This is the fasta formatted file with the peptide sequences used to predict
genes. The identifier of a sequence is taken from the description line 
with the pattern ``>(\S+)'' (characters between > and first white-space).
@item genome_*.fasta
These are the genomic files. They have to start with ``genome_'' in the front.
Each file should only contain a single genomic sequence in fasta format.
@item cdnas.fasta
The cdnas of the peptides used for prediction. This is a fasta formatted file.
Sequences have to use the same identifiers as in the @file{peptides.fasta} file.
The sequences may contain UTRs. This file is used for calculating kaks.
@item exon_boundaries_reference
File with exon boundaries of the proteins used to predict genes. This is
a tab separated table with the following fields:

@enumerate
@item Identifier of peptide
@item Identifier of chromosomal segment
@item Strand
@item Phase of the exon
@item Exon number
@item Start position  (in nucleotides) of the exon
@item End position +1 (in nucleotides) of the exon
@item Start position  (in bp) of the exon on chromosomal segment.
@item End position +1 (in bp) of the exon on chromosomal segment.
@end enumerate

Example:
@verbatim
ENST00000000233 chr7    1       0       1       0       67      126782947       126783108
ENST00000000233 chr7    1       2       2       67      148     126783625       126783706
ENST00000000233 chr7    1       2       3       148     258     126784027       126784137
ENST00000000233 chr7    1       0       4       258     330     126784608       126784680
ENST00000000233 chr7    1       0       5       330     456     126785505       126785631
ENST00000000233 chr7    1       0       6       456     543     126785755       126786242
@end verbatim

@end table

@c ------------------------------------------------------
@node  Using the pipeline, Analysing the result, Installation, Top
@chapter Using the pipeline

@menu
* Overview of Usage::           
* Preparation::                 
* Running the pipeline::        
@end menu

@node Overview of Usage, Preparation, Using the pipeline, Using the pipeline
@section Overview of Usage

The pipeline is implemented in a Makefile, that is the input to unix make
@xref{Top, make, , make}. The syntax to run a command is 
@verbatim
make <command>
@end verbatim
 
This section describes the various commands that are available.

@node  Preparation, Running the pipeline, Overview of Usage, Using the pipeline
@section Preparation

The first step in the pipeline is to setup the temporary working directory
and to create the schema and tables in the relation database. All this is done
with running the command

@verbatim
make prepare
@end verbatim

If you receive errors at this stage, the pipeline has not been setup correctly.
See @ref{Troubleshooting} for help.

@node  Running the pipeline,  , Preparation, Using the pipeline
@section Running the pipeline

Running the pipeline is as easy as typing a 

@verbatim
nice -19 nohup make all >& pipeline.out
@end verbatim.

in the working directory.

The above command will run the pipeline from the start till the end. Commands that are
executed by make are echoed on standard output, error messages are on standard
error. With the above command, both are redirected to the file @file{pipeline.out}.
The pipeline itself keeps a log of what it does in the file @file{log}
in the working directory.

On error, the pipeline will halt. Examine the file pipeline.out for the reasons.

@menu
* Step1::                       Masking of the protein sequences  
* Step2::                       Clustering of the protein sequences  
* Step3::                       Running exonerate    
* Step4::                       Running TBLASTN      
* Step5::                       Collating putative genic regions.  
* Step6::                       Predicting genes for representative sequences  
* Step7::                       Predicting genes for redundant sequences  
* Step8::                       Predicting genes for member sequences  
* Step9::                       Analysing the predictions  
* Step10::                      Quality control of predictions  
@end menu

@node Step1, Step2, Running the pipeline, Running the pipeline
@subsection Step1

@pindex cd_hit
@vindex PARAM_OPTIONS_SEG
Protein sequences are masked using the @command{seg}
program. The variable @var{PARAM_OPTIONS_SEG} contains the options
for seg.

@node Step2, Step3, Step1, Running the pipeline
@subsection Step2

@pindex cd_hit
@vindex PARAM_OPTIONS_CDHIT

Protein sequences are clustered using the program 
@uref{http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=pubmed&dopt=Abstract&list_uids=11836214, @command{cd-hit}}:

@cite{Li W, Jaroszewski L, Godzik A. 
Tolerating some redundancy significantly speeds up clustering of large protein databases.
Bioinformatics. 2002 Jan;18(1):77-82.}

This program removes redundant sequences by sorting the sequences by
length. In order of decreasing length, sequences are eliminated that
match in their entirety to a longer sequence with more than 90% identity.

The variable @var{PARAM_OPTIONS_CDHIT} contains the options
for @command{cd-hit}.

@node Step3, Step4, Step2, Running the pipeline
@subsection Step3

@pindex exonerate
@vindex PARAM_EXONERATE_NUMJOBS
@vindex PARAM_EXONERATE_GENOME_CHUNKSIZE
@vindex PARAM_EXONERATE_PEPTIDES_CHUNKSIZE
@vindex PARAM_EXONERATE_EXTEND
@vindex PARAM_EXONERATE_MAX_PERCENT_OVERLAP
@vindex PARAM_EXONERATE_MIN_SCORE
@vindex PARAM_EXONERATE_MIN_COVERAGE_QUERY
@vindex PARAM_EXONERATE_MAX_MATCHES
@vindex PARAM_EXONERATE_OPTIONS

The program @command{exonerate} is used to scan for putative genic regions. 
Documentation for exonerate can be found on its 
@uref{http://www.ensembl.org/Docs/wiki/html/EnsemblDocs/Exonerate.html,homepage}.

This step is controlled by the following options:

@verbatim
################################################
## parameters for exonerate
## number of jobs to submit to queue
PARAM_EXONERATE_NUMJOBS?=100
## number of nucleotides per genomic file.
PARAM_EXONERATE_GENOME_CHUNKSIZE=2000000 
## number of peptides per file.
PARAM_EXONERATE_PEPTIDES_CHUNKSIZE=4000
## overlapping residues of genomic files
PARAM_EXONERATE_EXTEND=100000
## maximum overlap for not turning on conflict resolution
PARAM_EXONERATE_MAX_PERCENT_OVERLAP?=0.2
## minimum score of a successfull match
PARAM_EXONERATE_MIN_SCORE?=80
## minimum coverage of query of a successfull match
PARAM_EXONERATE_MIN_COVERAGE_QUERY?=10
## number of matches per query to consider (0 = all)
PARAM_EXONERATE_MAX_MATCHES?=0
## exonerate options
PARAM_EXONERATE_OPTIONS=-m p2g --forcegtag TRUE  --bestn 200 --maxintron 1000 --proteinwordthreshold 3 --proteinhspdropoff 5 --proteinwordlen 5 
*end verbatim

The option @var{PARAM_EXONERATE_OPTIONS} in particular determines
the sensitivity of the search. Increasing the sensitivity
(increase @var{proteinwordthreshold}, @var{proteinhspdropoff},
decrease @{proteinwordlen}) greatly increase running time. If set
too permissive and depending on lengths of the queries/genomic fragments,
certain runs might not finish at all.

Exonerate occasionally produces segmentation faults.

@subsubsection Step3.0
The data is perpared for parallel execution. The file
with the representative peptide sequences, @file{peptides.clustered}
is split into chunks of size @var{PARAM_EXONERATE_PEPTIDES_CHUNKSIZE}.

The genomic fragments are split into segments of @var{PARAM_EXONERATE_GENOME_CHUNKSIZE} bases.
Adjacent segments overlap by @var{PARAM_EXONERATE_EXTEND} bases.

@subsubsection Step3.1
In this step, a parallel @program{make} is executed. The variable 
@var{PARAM_EXONERATE_NUMJOBS} determines, how many jobs are submitted to 
the cluster in parallel. 

The output of exonerate is directly parsed into the prediction format
used throughout the pipeline @xref{Prediction format}. If a query matches
to adjacent regions in a chromosomal segment, the regions are combined.

@subsubsection Step3.2

@pindex{regions2predictions.py}

In this step, exonerate predictions are parsed to produce putative genic regions.

The script @command{regions2predictions.py} takes various predictions from
exonerate.

@subsubsection Additional targets

@table @code
@item step3.rollback
Removes all files from this step. A new make all will start
this step from scratch.
@item step3.clean
Removes all temporary files from this step. This reduces the
number of files in the directory. Only use this command after
the step has finished completely and the pipeline has entered
the next stage.
@end table

@node Step4, Step5, Step3, Running the pipeline
@subsection Step4


@node Step5, Step6, Step4, Running the pipeline
@subsection Step5


@node Step6, Step7, Step5, Running the pipeline
@subsection Step6


@node Step7, Step8, Step6, Running the pipeline
@subsection Step7


@node Step8, Step9, Step7, Running the pipeline
@subsection Step8

@node Step9, Step10, Step8, Running the pipeline
@subsection Step9

@node Step10,  , Step9, Running the pipeline
@subsection Step10


@c ------------------------------------------------------
@node Analysing the result, Implementation, Using the pipeline, Top
@chapter Analysing the results


@c ------------------------------------------------------
@menu
* Troubleshooting::             
@end menu

@node  Troubleshooting,  , Analysing the result, Analysing the result
@section Troubleshooting

@c ------------------------------------------------------
@node Implementation, Command and Variable Index, Analysing the result, Top
@chapter Implementation

@c ------------------------------------------------------
@node    Command and Variable Index, Concept Index, Implementation, Top
@chapter Command and Variable Index

@node    Concept Index,  , Command and Variable Index, Top
@chapter Concept Index
@unnumbered Concept Index

@printindex cp
@contents
@bye

